{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multithreading & Multiprocessing in python\n",
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Understanding\n",
    "\n",
    "Art of doing multiple task at the same time. 'Same time' is bit of an confusing term. If there are multiple processors then we can actually do multiple task simultaneously. Else we can atleast take advantage of process which are I/O blocked.  \n",
    "\n",
    "## Multi-Threading\n",
    "Multithreading in Python allows to take advantage when our operation is blocked by I/O. Like calling multiple webservices or database calls or network calls at the same time.\n",
    "\n",
    "## Multi-Processing\n",
    "If we want to take advantage of the multiple processors present on the machine we would need to use multiprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example without multithreading or multiprocessing\n",
    "\n",
    "In the below example we are executing code without multithreading.\n",
    "It will take 4 seconds as there are two functions which take 2 seconds each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "def calculate_raise_to(x, y):\n",
    "    time.sleep(2)\n",
    "    print([i ** y for i in x])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input = [1,2,3,4,5,6,7]\n",
    "    start = time.time()\n",
    "    calculate_raise_to(input, 2)\n",
    "    calculate_raise_to(input,3,)\n",
    "    print('time taken', round(time.time()-start,2) , 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with multithreading\n",
    "\n",
    "In the below example we are executing code with multithreading.\n",
    "It will take 2 seconds as there are two functions which are running in parallel and waiting also in parallel, due to the above it finishes in just more than 2 seeconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "def calculate_raise_to(x, y):\n",
    "    time.sleep(2)\n",
    "    print([i ** y for i in x])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input = [1,2,3,4,5,6,7]\n",
    "    start = time.time()\n",
    "    t1 = threading.Thread(target = calculate_raise_to, args=(input,2,))\n",
    "    t2 = threading.Thread(target = calculate_raise_to, args=(input,3,))\n",
    "    t1.start()\n",
    "    t2.start() \n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    print('time taken', round(time.time()-start,2) , 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables with multithreading\n",
    "\n",
    "Multithreading happens with in the same process, so the global variables are shared accross various threads.\n",
    "\n",
    "Check in the below example that the global variables would be filled up with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "global_results = []\n",
    "\n",
    "def calculate_raise_to(x, y):\n",
    "    global global_results\n",
    "    results.append([i ** y for i in x])\n",
    "    print('results is',global_results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input = [1,2,3,4,5,6,7]\n",
    "    t1 = threading.Thread(target = calculate_raise_to, args=(input,2,))\n",
    "    t1.start()\n",
    "    t1.join()\n",
    "    print('results is',global_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with multithreading\n",
    "\n",
    "In the below example we are executing code with multiprocessnig. It will take 2 seconds as there are two functions which are running on seperate processes which take 2 seconds each.\n",
    "\n",
    "If you take a snapshot of process before and after the process are started, then we can notice that we have a couple of process added and then removed once the processes are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing as mp\n",
    "import psutil\n",
    "\n",
    "def print_python_processes(msg):\n",
    "    print(msg)\n",
    "    for pid in psutil.process_iter():\n",
    "        if('python' in str(pid)):\n",
    "            print(str(pid))\n",
    "        \n",
    "def calculate_raise_to(x, y):\n",
    "    time.sleep(5)\n",
    "    print([i ** y for i in x])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input = [1,2,3,4,5,6,7]\n",
    "    start = time.time()\n",
    "    p1 = mp.Process(target = calculate_raise_to, args=(input,2,), name='Process_test_1_123')\n",
    "    p2 = mp.Process(target = calculate_raise_to, args=(input,3,), name='Process_test_2_abc')\n",
    "    print_python_processes('processes before we start our process')\n",
    "    p1.start()\n",
    "    p2.start() \n",
    "    print_python_processes('processes after we start our process')\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    print_python_processes('processes after we end our process')\n",
    "    print('time taken', round(time.time()-start,2) , 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables with multiprocessing\n",
    "\n",
    "Multiprocessing happens with on different process, so the global variables are on discrete/different address spaces and are not shared.\n",
    "\n",
    "Check in the below example that the global variables would ***not*** be filled up with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "results = []\n",
    "\n",
    "def calculate_raise_to(x, y):\n",
    "    global results\n",
    "    results.append([i ** y for i in x])\n",
    "    print('results is',results)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    input = [1,2,3,4,5,6,7]\n",
    "    t1 = multiprocessing.Process(target = calculate_raise_to, args=(input,2,))\n",
    "    t1.start()\n",
    "    t1.join()\n",
    "    print('results is',results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share memory between processes # 1\n",
    "\n",
    "As discribed above we would need something special to share information across processes.\n",
    "\n",
    "There are many ways, two of the them creating special arrays and values.\n",
    "\n",
    "Couple of examples below will explain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def calculate_raise_to(x, y, results6, value6):\n",
    "    print('result before sleep is', results6[:])\n",
    "    print('value before sleep is', value6.value)\n",
    "    time.sleep(10)\n",
    "    print('result after sleep is', results6[:])\n",
    "    print('value after sleep is', value6.value)\n",
    "    for index,value in enumerate(x):\n",
    "        results6[index] = results6[index] + (value ** y)\n",
    "    print('results before we return',results6[:]) \n",
    "    value6.value = 10\n",
    "    print('value before we return', value6.value)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input7 = [1,2,3,4,5]\n",
    "    value7 = multiprocessing.Value('i')\n",
    "    value7.value = 1\n",
    "    result7 = multiprocessing.Array('i', len(input7))\n",
    "    t1 = multiprocessing.Process(target = calculate_raise_to, args=(input7,2,result7,value7))\n",
    "    t1.start()\n",
    "    time.sleep(5)\n",
    "    result7[0] = 10\n",
    "    value7.value = 2\n",
    "    t1.join()\n",
    "    print('results after we return',result7[:]) \n",
    "    print('value after we return', value7.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share memory between processes # 2\n",
    "\n",
    "Queue is a very good way of sharing data between multiprocesses.\n",
    "\n",
    "Process 1 --> Queue --> Process 2\n",
    "\n",
    "Below is a sample producer consumer problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "def produce(queuep):\n",
    "    for i in range(100):\n",
    "        print('in produce', i)\n",
    "        queuep.put(i)\n",
    "\n",
    "\n",
    "def consume(queuec):\n",
    "    for i in range(100):\n",
    "        print('in consume', queuec.get())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    queue = multiprocessing.Queue(10)\n",
    "    consume_process = multiprocessing.Process(name='consumer', target=consume, args=(queue,))\n",
    "    produce_process = multiprocessing.Process(name='producer', target=produce, args=(queue,))\n",
    "    consume_process.start()\n",
    "    produce_process.start()\n",
    "    consume_process.join()\n",
    "    produce_process.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lock(synchronized memory) between processes\n",
    "\n",
    "Its sometimes required to ensure that there are no process are accessing the shared memory at the same time. If they do, they might end up in using stale state of the value and might missout on writes.\n",
    "\n",
    "Lock helps us ensuring that the code are that is using shared state (reading and writing both), would be executed in exclusitivity and will never end up using the sate state of the value.\n",
    "\n",
    "Below example demostrates the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "\n",
    "def deposit(dollard):\n",
    "    for k in range(100):\n",
    "        time.sleep(0.01)\n",
    "        dollard.value = dollard.value + 1\n",
    "\n",
    "\n",
    "def withdraw(dollarw):\n",
    "    for j in range(100):\n",
    "        time.sleep(0.01)\n",
    "        dollarw.value = dollarw.value - 1\n",
    "\n",
    "\n",
    "def deposit_lock(dollard, lockd):\n",
    "    for k in range(100):\n",
    "        time.sleep(0.01)\n",
    "        lockd.acquire()\n",
    "        dollard.value = dollard.value + 1\n",
    "        lockd.release()\n",
    "\n",
    "\n",
    "def withdraw_lock(dollarw, lockw):\n",
    "    for j in range(100):\n",
    "        time.sleep(0.01)\n",
    "        lockw.acquire()\n",
    "        dollarw.value = dollarw.value - 1\n",
    "        lockw.release()\n",
    "\n",
    "\n",
    "def test_without_lock():\n",
    "    error_count = 0\n",
    "    for i in range(10):\n",
    "        dollars = multiprocessing.Value('i', 200)\n",
    "        consume_process = multiprocessing.Process(name='deposit', target=deposit, args=(dollars,))\n",
    "        produce_process = multiprocessing.Process(name='withdraw', target=withdraw, args=(dollars,))\n",
    "        consume_process.start()\n",
    "        produce_process.start()\n",
    "        consume_process.join()\n",
    "        produce_process.join()\n",
    "        if dollars.value != 200:\n",
    "            error_count = error_count + 1\n",
    "    print('Error Count without lock', error_count)\n",
    "\n",
    "\n",
    "def test_with_lock():\n",
    "    error_count = 0\n",
    "    for i in range(10):\n",
    "        lock = multiprocessing.Lock()\n",
    "        dollars = multiprocessing.Value('i', 200)\n",
    "        consume_process = multiprocessing.Process(name='deposit', target=deposit_lock, args=(dollars, lock,))\n",
    "        produce_process = multiprocessing.Process(name='withdraw', target=withdraw_lock, args=(dollars, lock,))\n",
    "        consume_process.start()\n",
    "        produce_process.start()\n",
    "        consume_process.join()\n",
    "        produce_process.join()\n",
    "        if dollars.value != 200:\n",
    "            error_count = error_count + 1\n",
    "    print('Error Count with lock', error_count)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_without_lock()\n",
    "    test_with_lock()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pool for map-reduce paradigm\n",
    "\n",
    "Many times we have a task which are applied on a very big list/set, but the task can be excuted on the individual elements simultaneously and independently.\n",
    "\n",
    "For such problems we can use Pools from multiprocessing which can divide the work on available processors and then construct the result again\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
